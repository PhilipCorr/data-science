{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom half of https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d46ba3fd-26f1-4635-b2f9-fca916ff3066",
    "_uuid": "21f3ccd962d1556dc2346699d45a29e9ef791367"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n",
      "/Users/pcorr/anaconda3/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60326be1-82d1-4677-8ef8-da5b1eac475c",
    "_uuid": "adb496504ab8453ce2b4f91dd6e5f17cbdaf4f68"
   },
   "source": [
    "Let's load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "367e0329-7aeb-4f39-b1a9-d7395bdca993",
    "_uuid": "d6ea63db0ad0db09b25c35601391b71564601699"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/train.csv')\n",
    "test = pd.read_csv('../Data/test.csv')\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e171d134-bb33-4578-800c-2d65c2edf9c1",
    "_uuid": "c02f2a3e039aad543bc789188fe08422dd78f5c0"
   },
   "source": [
    "A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1a9da2ba-2c8c-466d-8ceb-cb1fdb4351a8",
    "_uuid": "0fccc28ade4d126b5279d52b0f24300f8c18e69b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c1ec2ba0-7bac-4983-8f68-850f26251eb6",
    "_uuid": "6b2ace3ea08492e59402dc2abcf65b99ff1a537e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a3d6737a-2e2b-4213-abf6-94dbefe3a660",
    "_uuid": "43a424c31b40cc7ba4b622fb915846ae00c1f3b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c44d877-8fff-41a8-95dc-cd4ddebfa337",
    "_uuid": "68e1ad174df4fd40bb6965d9d46aaf42b8fa39c8"
   },
   "source": [
    "The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes.\n",
    "\n",
    "For this particular problem, Kaggle has specified multi-class log-loss as evaluation metric. This is implemented in the follow way (taken from: https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "01cee065-8733-44d6-b348-7124e178c6e5",
    "_uuid": "ecd0f67a617ae3c03f2386dd722be343c2384ff3"
   },
   "source": [
    "In NLP problems, it's customary to look at word vectors. Word vectors give a lot of insights about the data. Let's dive into that.\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it. I am a fan of GloVe vectors, word2vec and fasttext. In this post, I'll be using the GloVe vectors. You can download the GloVe vectors from here `http://www-nlp.stanford.edu/data/glove.840B.300d.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a815fde2-a243-4216-929c-2f134ef0663c",
    "_uuid": "8a2edafbcc6c9c0da2ccc1307f7e4ba75950870b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1587411it [01:33, 16949.93it/s]ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-9d8793b73bea>\", line 6, in <module>\n",
      "    values = line.split()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 736, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 705, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/Users/pcorr/anaconda3/envs/py36/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../Data/glove.42B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe4e61d9-28d9-4171-a589-b1effaaa1116",
    "_uuid": "a7445bd63a857a4619043f8916384d038a9f5c54"
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e79158dc-eb32-416a-ae91-d38612fca82f",
    "_uuid": "c45f5aeb64f55b6affa932b2f0048b058264518e"
   },
   "outputs": [],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac1c5c55-a5ca-4f1f-a5cc-ff0964f887d8",
    "_uuid": "e25a2275ca165dc4e90589f4778c9b84585448a1"
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f2b9d5f-d31c-4ed2-ba7f-43833bd63ddd",
    "_uuid": "a295bf47c480c9112dee20e5691594b379f4512e"
   },
   "source": [
    "Let's see the performance of xgboost on glove features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb5bb25f-eac2-457d-b69d-e236b5d47953",
    "_uuid": "e35e50e524ae353dda1614c758ff59901e5af23c"
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "41fa545d-0576-4539-bc8c-fb03aef61440",
    "_uuid": "8ab2c7e8eb07e9eb6b7b21d36dcd9b366672cac2"
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "12187a0c-ec9d-46ba-84bb-d99ff9850cf0",
    "_uuid": "f2e77eaf67811c2b8e1465974030f2797ba786ba"
   },
   "source": [
    "we see that a simple tuning of parameters can improve xgboost score on GloVe features! Believe me you can squeeze a lot more from it.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "But this is an era of deep learning! We cant live without training a few neural networks. Here, we will train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef1aecb1-2cca-48aa-86f5-ef3f27682924",
    "_uuid": "415001df909a6cdfbb23bd18f042ae98207b736c"
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2848ff6-79c2-428e-a932-d9b0295f1d48",
    "_uuid": "bee826d9cf5afb4524d00a539870fc3cff9b601a"
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b544f16b-624d-4b21-998e-aecc94295662",
    "_uuid": "d5294e86eeb398dc97a1ff7f2e09a9d26f5ebe16"
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5c443201-164e-4333-92d2-fea861f77609",
    "_uuid": "b7afdb2c41bb7ffc70a18fc65d403c519193647b"
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8b8a0c2-c086-498c-9663-f83b203af00c",
    "_uuid": "bbbd3b800ad85f9f7b3d964b55812f10323b1262"
   },
   "source": [
    "You need to keep on tuning the parameters of the neural network, add more layers, increase dropout to get better results. Here, I'm just showing that its fast to implement and run and gets better result than xgboost without any optimization :)\n",
    "\n",
    "To move further, i.e. with LSTMs we need to tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35e301c1-a17b-476d-a2c3-b606c9088338",
    "_uuid": "9457e2bf1b79db4a7567c1b04755e66b7828d493"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "xtest_seq = token.texts_to_sequences(xtest)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9cec8fb1-f218-4d65-aa1c-ab3357cb6b22",
    "_uuid": "63ac5c4c795f45474d263a8e21c01333c2234aa6"
   },
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f2fa769-d70e-43ef-b6ce-3b82fccaef32",
    "_uuid": "f978f21365eb8bf6684a70e3ae6fae2e8f4fff1b"
   },
   "outputs": [],
   "source": [
    "# # A simple LSTM with glove embeddings and two dense layers\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(word_index) + 1,\n",
    "#                      300,\n",
    "#                      weights=[embedding_matrix],\n",
    "#                      input_length=max_len,\n",
    "#                      trainable=False))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(3))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "67593280-81e1-44eb-9cd1-198822f316e8",
    "_uuid": "73d8a4dc4d0e245e979d7d142d1b5c7a4057aa7d"
   },
   "outputs": [],
   "source": [
    "# model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d84364cd-85de-4106-91e9-7eca06dab718",
    "_uuid": "01aa520c9ef6283a5399d2dd725d917c67d10213"
   },
   "source": [
    "We see that the score is now less than 0.5. I ran it for many epochs without stopping at the best but you can use early stopping to stop at the best iteration. How do I use early stopping?\n",
    "\n",
    "well, pretty easy. let's compile the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "953a3e6f-0052-4f71-9697-4c3a64846e17",
    "_uuid": "800379d15f6173755b1ec9b2e2d5b5a04509a1b3"
   },
   "outputs": [],
   "source": [
    "# # A simple LSTM with glove embeddings and two dense layers\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(word_index) + 1,\n",
    "#                      300,\n",
    "#                      weights=[embedding_matrix],\n",
    "#                      input_length=max_len,\n",
    "#                      trainable=False))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(3))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# # Fit the model with early stopping callback\n",
    "# earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "#           verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf4c30e8-de2f-4787-b5ac-07a22089518e",
    "_uuid": "c2a05818abc8c70e3a8c4ab7118d3815a0b07147"
   },
   "source": [
    "One question could be: why do i use so much dropout? Well, fit the model with no or little dropout and you will that it starts to overfit :)\n",
    "\n",
    "Let's see if Bi-directional LSTM can give us better results. Its a piece of cake to do it with Keras :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51093329-081b-48d7-b29c-28a0b99f038f",
    "_uuid": "be091fd7888ef0239d7c76536e6a430ca3edca88"
   },
   "outputs": [],
   "source": [
    "# # A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(word_index) + 1,\n",
    "#                      300,\n",
    "#                      weights=[embedding_matrix],\n",
    "#                      input_length=max_len,\n",
    "#                      trainable=False))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.8))\n",
    "\n",
    "# model.add(Dense(3))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# # Fit the model with early stopping callback\n",
    "# earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "#           verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69875648-b716-457a-8db8-bcf625d7d3bf",
    "_uuid": "a1019cf81782debe55d16d55d90b4e1bc3076176"
   },
   "source": [
    "Pretty close! Lets try two layers of GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e427650c-11e9-422b-9e06-45e353721857",
    "_uuid": "5196303d6875a3d1cd9432631a6a0ecfe0be4d3f"
   },
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "lstm_model.add(SpatialDropout1D(0.3))\n",
    "lstm_model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "lstm_model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "lstm_model.add(Dense(1024, activation='relu'))\n",
    "lstm_model.add(Dropout(0.8))\n",
    "\n",
    "lstm_model.add(Dense(1024, activation='relu'))\n",
    "lstm_model.add(Dropout(0.8))\n",
    "\n",
    "lstm_model.add(Dense(3))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "lstm_model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = lstm_model.predict(xtest_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_df = pd.DataFrame(lstm_pred,columns=['EAP','HPL','MWS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_df['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_df = lstm_pred_df[['id','EAP','HPL','MWS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred_df = pd.DataFrame(clf_pred, columns=['EAP','HPL','MWS']) \n",
    "clf_pred_df['id'] = test['id']\n",
    "clf_pred_df = clf_pred_df[['id','EAP','HPL','MWS']]\n",
    "clf_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred_df.to_csv('../predictions/clf_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_df.to_csv('../predictions/lstm_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aeb72e91-0b0c-449e-a41b-5d2c1e6a6f26",
    "_uuid": "dfcac382d1c84f19ca12fa3a590ef2c67a30af75"
   },
   "source": [
    "Nice! Much better than what we had previously! Keep optimizing and the performance will keep improving.\n",
    "Worth trying: stemming and lemmatization. This is something I'm skipping for now.\n",
    "\n",
    "In the Kaggle world, to get a top score you should have an ensemble of models. Let's check a little bit of ensembling!\n",
    "\n",
    "\n",
    "## Ensembling\n",
    "\n",
    "Few months back I made a simple ensembler but I didn't have time to develop it fully. It can be found here: https://github.com/abhishekkrthakur/pysembler . I'm going to use some part of it here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "161adbaf-9eec-4e13-87ee-24e4621ee6e0",
    "_uuid": "1ec49abaf6fce8fee5a59f8fc56c85d0ed7626d5"
   },
   "source": [
    "Thus, we see that ensembling improves the score by a great extent! Since this is supposed to be a tutorial only I wont be providing any CSVs that you can submit to the leaderboard.\n",
    "\n",
    "I hope you like it! \n",
    "\n",
    "P.S.: If the response is good, I'll add more stuff in this! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
